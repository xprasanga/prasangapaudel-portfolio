---
title: "tidytuesday Exercise"
author: Prasanga Paudel
date: April 11, 2025
format:
  html:
    toc: false
    number-sections: true
    highlight-style: github
---


# Introduction

Quantitattive metrics of healthcare quality offer vital information about hospital performance and patient outcomes.The care_state dataset from TidyTuesday (April 8), which includes state-level hospital care quality metrics for various medical diseases, is analysed in this exercise. The dataset will provide an understanding of healthcare performance in the US by incorporating characteristics like patient scores, admission circumstances, measurement kinds, and time periods.


```{r, echo=FALSE, message=FALSE}
library(here)
library(dplyr)
library(tidyverse)
library(usmap)
library(tidyverse)
library(ggplot2)
library(tidymodels)  #rsample for splitting
library(glmnet)  #Lasso implementation
library(ranger)  #  random forest


```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cloud.r-project.org"))
```


# Importing dataset

We will import the dataset directly from Github using readr.

```{r, message=FALSE}
care_state <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-08/care_state.csv')
```

Now, we will look at the uppermost observations from tha dataset.

```{r}
head(care_state)
```

The variables are coded in the correct way. And we will modify them according to our needs if required.

# Exploratory Data Analysis

The dates look interesting, we might be able to use the duration from start to end as an information in our research. We will first look if there are multiple start dates and end dates or if all the dates are same.

Let's observe how start dates are distributed in our dataset.

```{r}
#counting the occurrences of each start_date
care_state %>%
  count(start_date, name = "frequency") %>%
  arrange(desc(frequency)) %>%  # sorting by most frequent
  print(n = Inf)  #showing all rows
```

We can observe that there are only four start dates although some of them have very high frequency and some of them have a low frequency.


Noe, let's observe how end dates are distributed in our dataset.

```{r}
#counting the occurrences of each end_date
care_state %>%
  count(end_date, name = "frequency") %>%
  arrange(desc(frequency)) %>%  #sorting by most frequent
  print(n = Inf)  #showing all rows
```

There are only two end dates.

So there is not much variation in this variables for exploration. Maybe other variables can much wider range of values. Lets observe how observations are distributed across states.
 
 
## Checking the most frequent condition in each state

Now we will see which of the condition is most frequent in each of the state and if there is a pattern among the states.

```{r}
#finding most frequent condition per state
top_conditions <- care_state %>%
  count(state, condition) %>%
  group_by(state) %>%
  slice_max(n, n = 1) %>%
  ungroup()

#creating basic map
plot_usmap(data = top_conditions, values = "condition") +
  scale_fill_brewer(palette = "Set2", 
                   name = "Most Common Condition",
                   na.value = "gray") +
  labs(title = "Most Frequent Admission Condition by State") +
  theme(legend.position = "right")
```

We can see that for every state Emergency is the most frequent condition.

## Checking the frequencies of different measures.

Now, we will observe the freqency of different values under the variable measure_name. We can look into only the top measures as well but since we are just exploring let's tabulate all availabe categories under the variable measure_name and tabulate their frequencies.

```{r}
#getting frequency counts for measure_name
measure_counts <- care_state %>%
  count(measure_name, sort = TRUE)  #sorting by most frequent

#showing the full table
print(measure_counts, n = Inf)  #n= all rows

```

We can observe that all the unique values have 56 observations, except the first value which has double observations.

## Average time patients spent in the emergency department before being sent home

We also have a category "Average time....before being sent home" in our variable measure_names. We will create a dataset to plot "Average time patients spent in the emergency department before being sent home" across states.

Let's create a dataset with measure_name having "Average time patients spent in the emergency department before being sent home" as only category.

```{r}

#creating filtered dataframe
avgtime <- care_state %>%
  filter(str_detect(measure_name, 
                   regex("Average time patients spent in the emergency department before being sent home", 
                        ignore_case = TRUE)))
```


Now, using the newly created datset, let's plot the "Average time patients spent in the emergency department before being sent home" across states.

But the values are average time for each patient, we will plot the overall average across all patients within a particular state.

```{r}
#calculating average time by state
state_avg_time <- avgtime %>% 
  group_by(state) %>%
  summarise(average_time_spent = mean(score, na.rm = TRUE)) %>%
  ungroup()

#creating the map visualization
plot_usmap(data = state_avg_time, values = "average_time_spent", color = "white") +
  scale_fill_viridis_c(
    name = "Average Time (minutes)", 
    option = "plasma",
    direction = -1,
    na.value = "grey90"
  ) +
  labs(
    title = "Average ER Wait Time Before Discharge",
    subtitle = "Time spent in emergency department before being sent home"
  ) +
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  )

```

We can observe that there is variation across states with central US states showing a low time spent in emergency department before being sent home.



## Influenza Vaccination Rates Among Healthcare Workers across states.

We will now plot influenza vaccination rates among healthcare workers across states using the information in the variable vaccination_rate. We will create a new dataset again by filtering "Healthcare workers given influenza vaccination" category from the measure_name variable.

```{r}
#filtering vaccination data
flu_vax <- care_state %>%
  filter(str_detect(measure_name, 
                   "Healthcare workers given influenza vaccination")) %>%
  group_by(state) %>%
  summarise(vaccination_rate = mean(score, na.rm = TRUE))

#creating the map 
plot_usmap(data = flu_vax, 
           values = "vaccination_rate",
           color = "white") +
  scale_fill_distiller(
    name = "Vaccination Rate (%)",
    palette = "Spectral",
    direction = -1,  # Red = low, Green = high
    na.value = "grey90",
    limits = c(0, 100)  # 0-100% scale
  ) +
  labs(title = "Influenza Vaccination Rates Among Healthcare Workers",
       subtitle = "Higher percentages indicate better coverage") +
  theme(legend.position = "right")
```

We an observe that there is variation across different states.

## Average values for "score" across states

Now, we will try to see the average values for "score" across states. Note that "score" contains different types of values including days spent at hospital, use of opoids, and different proportion such as vaccinations. It is not a good idea to just use the average across all measure_name as they contain different kind of information but as this project only focuses on the learning the procedure, we can overlook this aspect.

```{r}
condition_corr <- care_state %>%
  group_by(condition) %>%
  summarise(mean_score = mean(score, na.rm = TRUE)) %>%
  arrange(desc(mean_score))

ggplot(condition_corr, aes(x = reorder(condition, mean_score), y = mean_score)) +
  geom_col(fill = "steelblue") +
  geom_text(aes(label = round(mean_score, 1)), vjust = -0.5) +
    scale_x_discrete(labels = function(x) str_wrap(x, width = 1)) +
  labs(title = "Average Score by Medical Condition",
       x = "Conditions and Categories",
       y = "Average Score") +
  theme_minimal() +
   theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
 axis.text.x = element_text(angle = 0, hjust = 0.5))

```

We can observe that medical condition and scores are also related to each other.

## Relation between enrollment duration and score


We will first convert days to numeric so that it is easy to plot it on the axes.

```{r}
care_state <- care_state %>% 
  mutate(days = as.numeric(end_date - start_date))
```

Now, let's create the plot of duration at hospital vs score.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#creatig the plot days vs score
ggplot(care_state, aes(x = days, y = score)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Correlation Between Score and Duration (Days)",
       x = "Duration (Days)", 
       y = "Score") +
  theme_minimal()
```

The duration days also show some relation with the score.

# Project Objective

1. Use three different estimation methods to fit the primary model.

$score_i$ = $β_0$ + $β_1$ $days_i$ + $β_2$ $state_i$ + $β_3$ $condition_i$ + $β_4$ $measure_i$ <br>

2. Evaluate the performance of each estimation method using evaluation metrices. <br>

3. Use train/test and cross-validation method to check the robustness of the model. <br>


Although it is not advisable to run regression without knowing the actual relationship among the factors and a specific reason, we are just using this project to learn the process and we will overlook the fact that our actual model might not make any practical sense in some context. Saying this, we will still go ahead and check these hypothesis.

# Hypothesis

We observed that there is a significant relation between states and scores. Also, we observed that days, measure name and conditions also show some relation with the score.

We hypothesize that the factors "measure name" "days", "state" and "condition" influence the score.

#  Analysis 

We observed that the start date and end dates do not vary much across the observations and are quite similar. There are four categories of start date and two categories of end date. We will create a variable "days" for duration days using the start date and the end date.


```{r}
care_state <- care_state %>%
  mutate(days = as.numeric(end_date - start_date))
```

We have now created the dataset with variable "days".

Now, we will prepare the dataset for our analysis. We will convert variables to factors and numeric based on the type of data they possess. We will also drop rows with missing values so that the sample size is same across all models and estimation methods in upcoming analysis. 

```{r}
#preparing the modeling dataset
data <- care_state %>%
  select(score, days, state, condition, measure_name) %>%
  mutate(
    days = as.numeric(days),  # numeric
    across(c(state, condition, measure_name), as.factor)  #converting categoricals to factors
  ) %>%
  drop_na()  #removing rows with missing values
```

## Traning and Testing

To test the robustness of our models we will divide the dataset into train/test dataset. We will use the train dataset for our entire analysis process and finnaly we will test if the models perform equally well in an unseen train dataset.

Now, let's split the data into train/train.

```{r}
#performing the 75/25 split
set.seed(123)  #for reproducibility
data_split <- initial_split(data, prop = 3/4)

#creating the training and train sets
train_data <- training(data_split)
test_data <- testing(data_split)
```

We will now use "days", "state", "condition", "measure_name" to predict the "score". We will use "Regression", "Lasso" and "Random Forest" models in the next sections.


## Regression

In this setion, we will perform a linear reression analysis. Although it is not a good idea to use variables with too many categories, but in this dataset the variables are too complex to categorize into smaller groups, so will proceed as it is.

The model is given as:

$score_i$ = $β_0$ + $β_1$ $days_i$ + $β_2$ $state_i$ + $β_3$ $condition_i$ + $β_4$ $measure_i$

```{r}
#fitting linear regression
lm_fit <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(score ~ days + state + condition + measure_name, 
      data = train_data)
```

Now, due to a very high number of categories within the variable measure, we will get a lot of predictor variables in our regression model. But, **seeking causality is not our objective here**. So, we will just list the most significant factors from the regression result and move on to the more important part: the performance of our model.

The most significant factors are:

```{r}
#getting model summary
tidy(lm_fit) %>% 
  arrange(p.value)  # Show most significant predictors first
```

Now, lets look into the performance of our model which is more important for us for this project.

```{r, warning=FALSE}

#making predictions on train data
train_results <- train_data %>% 
  bind_cols(predict(lm_fit, new_data = train_data))

#evaluating performance
metrics <- train_results %>% 
  metrics(truth = score, estimate = .pred)
```


The (Root Mean Squared Errors) RMSE, $R^2$ value, and (Mean Absolute Deviance) MAE for the OLS model is given below.

```{r}
print(metrics)
```

We can observe that the RMSE value is 34.9 and the R-square value is 0.876. 


Now, let's perform a visual test for model fit using observed and predicted values.

```{r}
# Observed vs Predicted plot
ggplot(train_results, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, linetype = "dashed") +
  labs(title = "Linear Regression Performance",
       x = "Observed Scores", 
       y = "Predicted Scores") +
  theme_minimal()
```

The observed-predicted plot also shows a good fit for the datapoints as datapoints lie close to the fitted line.

## LASSO regression

Now, lets fit the same model using lasso-regression. Lasso (Least Absolute Shrinkage and Selection Operator) regression is a statistical tool, and also a type of linear regression that adds a penalty (L1 regularization) to shrink coefficients. This can force ciefficients of some less important predictors to become exactly zero, and hence performing an automatic feature selection, preventing overfitting, and simpliflying models especially with many variables.

The model for lasso is also the same model used in OLS, we just add a penalty value of 0.1 on this estimation process.

$score_i$ = $β_0$ + $β_1$ $days_i$ + $β_2$ $state_i$ + $β_3$ $condition_i$ + $β_4$ $measure_i$

Fitting the model using LASSO.

```{r}

#setting up Lasso model specification
lasso_spec <- linear_reg(
  penalty = 0.1,  #Regularization strength 
  mixture = 1     # 1 = Lasso
) %>% 
  set_engine("glmnet")

#fitting Lasso model
lasso_fit <- lasso_spec %>% 
  fit(score ~ days + state + condition + measure_name, 
      data = train_data)

```

Now, lets look into the coefficients to see if any factors were dropped by LASSO regularization.

```{r}
# coefficients (non-zero ones only)
tidy(lasso_fit) %>% 
  filter(estimate != 0) %>% 
  arrange(desc(abs(estimate)))

```

Interestingly, our OLS regression has 78 predictors in the final estimated model but our LASSO regression has only 73 predictors which means some variables were dropped by our LASSO model. If we want to know which variable was dropped, we can simply ask R to print the full regression result and then we can identify which factor was considered unnecessary by LASSO regression. But, as discussed earlier, we are not here for causality.

Let's look into the performance of our LASSO regression.

```{r}

#making predictions
train_results <- train_data %>% 
  bind_cols(predict(lasso_fit, new_data = train_data))

#evaluating performance
lasso_metrics <- train_results %>% 
  metrics(truth = score, estimate = .pred)

```

The (Root Mean Squared Errors) RMSE, $R^2$ value, and (Mean Absolute Deviance) MAE for the LASSO model is given below.

```{r}
print(lasso_metrics)
```

We can observe a RMSE value of 35, which is smaller than what we observed for the OLS model. The R-squared value is 0.875 which is quite simillar to the regression model. 

Now, lets also check the model fit graphically.

```{r}
#Observed vs Predicted plot
ggplot(train_results, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.5, color = "darkorange") +
  geom_abline(slope = 1, linetype = "dashed") +
  labs(title = "Lasso Regression Performance",
       subtitle = paste("Penalty =", lasso_fit$spec$args$penalty),
       x = "Observed Scores", 
       y = "Predicted Scores") +
  theme_minimal()
```

The observed-predicted plot also shows a good fit as observed for the regression model.

## Random Forest

The model for Random Forest is also the same model used in OLS and LASSO technique. But for Random Forest we will add:

$trees=500$, which stands for default number of decision trees used when implementing the Random Forest,

$mtry=3$, which sets the number of predictors that will be randomly sampled at each tree split, and 

$minimal-node-size=5$, which controls the minimum number of data points required to be in a node for it to be split any further during the tree-building process.

$score_i$ = $β_0$ + $β_1$ $days_i$ + $β_2$ $state_i$ + $β_3$ $condition_i$ + $β_4$ $measure_i$


Lets fit the equation using Random Forest model.

```{r}

#setting up and fit Random Forest
rf_spec <- rand_forest(
  mtry = 3,  #
  trees = 500,
  min_n = 5
) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

rf_fit <- rf_spec %>% 
  fit(score ~ days + state + condition + measure_name, 
      data = train_data)
```

Now, lets create a figure to see the important variables suggested by the random Forest model.

```{r}

#extracting variable importance 
importance_df <- rf_fit$fit$variable.importance %>% 
  enframe(name = "variable", value = "importance") %>% 
  arrange(desc(importance))

#plotting importance 
ggplot(importance_df, aes(x = reorder(variable, importance), y = importance)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(x = NULL, y = "Importance", 
       title = "Random Forest Variable Importance") +
  theme_minimal()
```

The results from random forest model suggets a relatively high importance of measure_name in predicting the score for the particular observation. But, lets not focus on causal relations here. We are here to compare the model performance across different estimation techniques.

Let's look into the performance metrices of our RF model

```{r}
#making predictions and evaluate RF model
rf_results <- train_data %>% 
  bind_cols(predict(rf_fit, new_data = train_data))

rf_metrics <- rf_results %>% 
  metrics(truth = score, estimate = .pred)

print(rf_metrics)
```

We can observe that the RMSE of a random forest model is 26.6 while the R-square is 0.929. If we compare the three models, the Random Forest model has the lowest RMSE and the highest R-squared value. Thus, RF performs the best.

Let's also perform a visual test for the model fit.

```{r}
# Observed vs Predicted plot
ggplot(rf_results, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.6, color = "darkgreen") +
  geom_abline(slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Random Forest: Observed vs Predicted",
       x = "Actual Score", y = "Predicted Score") +
  theme_minimal()
```

We can observe that the random forest model shows a very small difference between the observed and the predicted with values clustered around the fitted line. Hence, it is a better model.


To summarize, lets tabulate the performance metrices of all three models: OLS regression, LASSO regression, and Random Forest model, all in a single table.


```{r}

#comparing all models
bind_rows(
  linear_reg = metrics,
  lasso = lasso_metrics,
  random_forest = rf_metrics,
  .id = "model"
) %>% 
  select(model, .metric, .estimate) %>% 
  pivot_wider(names_from = .metric, values_from = .estimate)

```

It is clearly visible that Random Forest model has the lowest RMSE value and the highest R-squared value. MAE is also the smallest for the RF model. All three metrices show that Random Forest model provides the best fit for our equation.

# CV approach

As we know, a cross-validation approach systematically evaluates a machine learning model's performance by repeatedly splitting data into training and validation sets, training the model on the training portion, and testing its generalization on the unseen validation portion, with results averaged to provide a robust estimate of how it performs on new data, preventing overfitting by ensuring every data point serves as a test case at least once, typically using methods like k-fold and leave-one-out. In our case, we will follow the k-fold technique.

Now, as a robustness test, lets use the Cross-validation approach to test our results. We will use 5-folds for this project.

```{r, warning=FALSE, message=FALSE}
#creating CV folds from training data (10-fold by default)
set.seed(123)
cv_folds <- vfold_cv(train_data, v = 5)  # 5-fold CV

#defining recipe (preprocessing)
model_recipe <- recipe(score ~ days + state + condition + measure_name, 
                      data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%  # Convert factors to dummy variables
  step_normalize(all_numeric_predictors())  # Center/scale numeric predictors

#model specifications
# Linear Regression
lm_spec <- linear_reg() %>% 
  set_engine("lm")

# Lasso Regression
lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>% 
  set_engine("glmnet")

# Random Forest
rf_spec <- rand_forest(mtry = 3, trees = 500, min_n = 5) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

# Workflows
lm_wf <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(lm_spec)

lasso_wf <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(lasso_spec)

rf_wf <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(rf_spec)

#fitting Models with CV
lm_res <- fit_resamples(
  lm_wf,
  resamples = cv_folds,
  metrics = metric_set(rmse, rsq, mae)
)

lasso_res <- fit_resamples(
  lasso_wf,
  resamples = cv_folds,
  metrics = metric_set(rmse, rsq, mae)
)

rf_res <- fit_resamples(
  rf_wf,
  resamples = cv_folds,
  metrics = metric_set(rmse, rsq, mae)
)

```


Now, we will use the performance data obtained from the 5-fold and take the average. We will do this for each estimation technique.


Let's start with the performance of OLS model in the CV test.

```{r}
collect_metrics(lm_res)
```

We can observe that the RMSE values for the Linear regressions are quite consistent and show a comparitively low standard deviation. These RMSE values are also quite similar to the ones we obtained in our whole train dataset which was 34.9


Now, lets look into the performance of LASSO technique in the CV test.

```{r}
collect_metrics(lasso_res)
```

We can observe that the RMSE values for the LASSO regressions are also quite consistent and show a comparitively low standard deviation. These RMSE values are also quite similar to the ones we obtained in our whole train dataset which was 35. The R-squared values are also quite similar to our previous value.

Finally, lets look into the performance of our Random Forest technique in the CV test.

```{r}
collect_metrics(rf_res)
```

In the case of the Random Forest model, we can observe that there is a very high increase in the RMSE value, almost the double of the previous value. This implies that the model performance of our Random forest model in the  train dataset can not be trusted. We will now see if this pattern of inconsistency also exists in the test dataset or not.


# Test dataset
 
 In this section, we will use the estimated model from our training dataset to see if the model can accurately predict when test dataset is used.
 
## OLS Regression (Test dataset)

We will start with our OLS prediction We will use the model estimated from our train dataset and calculte the performance metrices.

```{r,message=FALSE, warning=FALSE}
#making predictions on test data
test_results <- test_data %>% 
  bind_cols(predict(lm_fit, new_data = test_data))

#  performance
test_metrics <- test_results %>% 
  metrics(truth = score, estimate = .pred)

#  performance metrics
print(test_metrics)

```

We can observe that the RMSE value for the linear regression has increased by almost 50% but the R-square value is somewhat consistent with the train results.


Let's also observe the model fit graphically for the test dataset.

```{r}
# Observed vs Predicted plot
ggplot(test_results, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, linetype = "dashed") +
  labs(title = "Linear Regression Performance",
       x = "Observed Scores", 
       y = "Predicted Scores") +
  theme_minimal()
```

The predicted-observed plot shows a good fit for the datapoints.


## LASSO (TEST dataset)

Now, we will perfom the LASSO prediction on the test dataset as well. As with OLS regression, we will use the estimated model from the train dataset to predict using the test dataset and calculate the performance metrices.

```{r}
#predictions
test_results <- test_data %>% 
  bind_cols(predict(lasso_fit, new_data = test_data))

#performance
lasso_metrics_test <- test_results %>% 
  metrics(truth = score, estimate = .pred)

#print performance
print(lasso_metrics_test)

```

The LASSO regression also shows an almost 50% increase in RMSE values however the RMSE values are slightly greater than the Linear regression model although the difference is very small almost dismissable.


Let's also observe the model fit graphically for the test dataset.

```{r}
# Observed vs Predicted plot
ggplot(test_results, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.5, color = "darkorange") +
  geom_abline(slope = 1, linetype = "dashed") +
  labs(title = "Lasso Regression Performance on test data",
       subtitle = paste("Penalty =", lasso_fit$spec$args$penalty),
       x = "Observed Scores", 
       y = "Predicted Scores") +
  theme_minimal()
```


We can observe that the predicted-observed plot show a good fit for the datapoints.

## Random Forest (Test dataset)

Finally, we will perfom the Random Forest prediction on the test dataset as well. As with OLS prediction and LASSO prediction, we will use the estimated model from the train dataset to predict using the test dataset and calculate the performance metrices.

```{r}
#  predictions 
rf_results_test <- test_data %>% 
  bind_cols(predict(rf_fit, new_data = test_data))

rf_metrics_test <- rf_results_test %>% 
  metrics(truth = score, estimate = .pred)

print(rf_metrics_test)
```

We can observe that the  RMSE value for the random forest is even bigger, increasing by almost 100%, the R-squared value has also decreased.


Let's also observe the model fit graphically for the test dataset.

```{r}
# Observed vs Predicted plot
ggplot(rf_results_test, aes(x = score, y = .pred)) +
  geom_point(alpha = 0.6, color = "darkgreen") +
  geom_abline(slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Random Forest: Observed vs Predicted",
       x = "Actual Score", y = "Predicted Score") +
  theme_minimal()
```

We can observe that the random model also shows agood fit for the datapoints.


Now, to summarize, let's tabulte the performance of all three models.

```{r}
#comparing all three models
bind_rows(
  linear_reg = test_metrics,
  lasso = lasso_metrics_test,
  random_forest = rf_metrics_test,
  .id = "model"
) %>% 
  select(model, .metric, .estimate) %>% 
  pivot_wider(names_from = .metric, values_from = .estimate)

```

# Comparision Table

```{r}
#creating comprehensive comparison table with fixed CV metrics
performance_summary <- bind_rows(
  #training results
  bind_rows(
    linear_reg = metrics,
    lasso = lasso_metrics,
    random_forest = rf_metrics,
    .id = "model"
  ) %>% mutate(data = "Training"),
  
  # CV results
  bind_rows(
    linear_reg = show_best(lm_res, metric = "rmse") %>% slice(1) %>% mutate(.metric = "rmse"),
    linear_reg = show_best(lm_res, metric = "rsq") %>% slice(1) %>% mutate(.metric = "rsq"),
    linear_reg = show_best(lm_res, metric = "mae") %>% slice(1) %>% mutate(.metric = "mae"),
    lasso = show_best(lasso_res, metric = "rmse") %>% slice(1) %>% mutate(.metric = "rmse"),
    lasso = show_best(lasso_res, metric = "rsq") %>% slice(1) %>% mutate(.metric = "rsq"),
    lasso = show_best(lasso_res, metric = "mae") %>% slice(1) %>% mutate(.metric = "mae"),
    random_forest = show_best(rf_res, metric = "rmse") %>% slice(1) %>% mutate(.metric = "rmse"),
    random_forest = show_best(rf_res, metric = "rsq") %>% slice(1) %>% mutate(.metric = "rsq"),
    random_forest = show_best(rf_res, metric = "mae") %>% slice(1) %>% mutate(.metric = "mae"),
    .id = "model"
  ) %>% 
    select(model, mean, .metric) %>% 
    rename(.estimate = mean) %>% 
    mutate(data = "CV"),
  
  # Test results
  bind_rows(
    linear_reg = test_metrics,
    lasso = lasso_metrics_test,
    random_forest = rf_metrics_test,
    .id = "model"
  ) %>% mutate(data = "Test")
) %>% 
  select(model, data, .metric, .estimate) %>% 
  pivot_wider(names_from = .metric, values_from = .estimate) %>% 
  arrange(model, data)

#printing the formatted table
performance_summary %>% 
  gt::gt() %>% 
  gt::fmt_number(columns = c(rmse, rsq, mae), decimals = 2) %>% 
  gt::tab_header(
    title = "Model Performance Summary",
    subtitle = "Comparison across training, cross-validation and test sets"
  )
```


# Conclusion

Based on the results we can conclude that the both the Linear regression model and the LASSO model show a very similar result and can be considered same. Although the Random Forest model showed a better performance in the train model, the CV results and the results from the test dataset speaks against the RF model. Therefore, we will conclude that Linear regression is a better model although the LASSO also shows an equally competititive result. The results from our Random Forest model showed that measure name, state and duration days had an impact on the score with measure name showing the greatest impact.
